{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
       " ('ViT-B-32-256', 'datacomp_s34b_b86k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'dfn2b'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'dfn2b'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-H-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-H-14-quickgelu', 'dfn5b'),\n",
       " ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
       " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
       " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
       " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
       " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
       " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
       " ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
       " ('ViT-B-16-SigLIP', 'webli'),\n",
       " ('ViT-B-16-SigLIP-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP-384', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
       " ('ViT-L-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-L-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA-336', 'laion2b'),\n",
       " ('ViT-H-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
       " ('nllb-clip-base', 'v1'),\n",
       " ('nllb-clip-large', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'v1'),\n",
       " ('nllb-clip-large-siglip', 'v1')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "b.extend(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[8, 36, 226, 207], [[3, 30.45317268371582], [8, -0.26999998092651367]]], [[152, 29, 102, 93], [[3, -30.45317268371582], [13, 0.0]]], [[81, 9, 134, 117], [[6, 0.26999998092651367], [4, -30.45317268371582]]], [[85, 18, 85, 84], [[9, 0.26999998092651367], [13, 0.0]]], [[9, 3, 236, 251], [[10, 7.0], [5, -9.0]]]]\n",
      "[\"a man is holding a green object and a quote \", \"a man holding a ball with a quote about it . \", \"a man holding a tennis ball in a field . \"]\n",
      "The source of Anime quotes & Manga quotes : Photo <PERSON>, Manga Quotes, Art Images, Fan Art, Thoughts, Think, Anime, Crying, Random\n",
      "{'image_emb': [tensor([ 1.1406, -0.1748,  0.2490,  ..., -1.0547,  1.8984,  0.5781],\n",
      "       dtype=torch.bfloat16), tensor([ 0.6719, -0.1592, -1.0938,  ..., -0.9453,  1.4688,  0.8438],\n",
      "       dtype=torch.bfloat16), tensor([ 0.7617,  0.1162, -0.6523,  ..., -0.5195,  1.6172,  0.5234],\n",
      "       dtype=torch.bfloat16), tensor([ 0.2090,  0.1826, -0.2266,  ..., -0.3730,  3.7656,  0.2988],\n",
      "       dtype=torch.bfloat16), tensor([ 0.9062, -0.2090, -0.2373,  ..., -0.7148,  1.1719,  0.2949],\n",
      "       dtype=torch.bfloat16)], 'text_emb': [tensor([ 0.5195, -0.1592,  0.2441,  ..., -0.3887,  0.6914,  0.6250],\n",
      "       dtype=torch.bfloat16), tensor([ 0.0024,  0.1523,  0.3555,  ..., -1.3750,  0.0413,  0.6641],\n",
      "       dtype=torch.bfloat16), tensor([-0.4355,  0.8281,  0.2207,  ..., -0.0952,  0.7227,  0.1719],\n",
      "       dtype=torch.bfloat16), tensor([ 0.1846, -0.0200, -0.1875,  ..., -0.7461,  0.3008,  0.1250],\n",
      "       dtype=torch.bfloat16)]}\n",
      "tensor([0.1943], dtype=torch.bfloat16)\n",
      "tensor([0.4668], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "import webdataset as wds\n",
    "import torch\n",
    "import io\n",
    "from torch.nn.functional import cosine_similarity\n",
    "dataset = wds.WebDataset(\"/home/user/data/cc12m_dr/00000.tar\")\n",
    "for sample in dataset:\n",
    "    print(sample['paug.json'].decode('utf-8'))\n",
    "    print(sample[\"syn_caps.json\"].decode('utf-8'))\n",
    "    print(sample[\"txt\"].decode('utf-8'))\n",
    "    buffer = io.BytesIO(sample['pth'])\n",
    "    tensor = torch.load(buffer)\n",
    "    print(tensor)\n",
    "    print(cosine_similarity(tensor['text_emb'][2].unsqueeze(0),tensor['text_emb'][3].unsqueeze(0)))\n",
    "    print(cosine_similarity(tensor['image_emb'][1].unsqueeze(0),tensor['image_emb'][3].unsqueeze(0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[8, 36, 226, 207], [[3, 30.45317268371582], [8, -0.26999998092651367]]], [[152, 29, 102, 93], [[3, -30.45317268371582], [13, 0.0]]], [[81, 9, 134, 117], [[6, 0.26999998092651367], [4, -30.45317268371582]]], [[85, 18, 85, 84], [[9, 0.26999998092651367], [13, 0.0]]], [[9, 3, 236, 251], [[10, 7.0], [5, -9.0]]]]\n",
      "[\"a man is holding a green object and a quote \", \"a man holding a ball with a quote about it . \", \"a man holding a tennis ball in a field . \"]\n",
      "The source of Anime quotes & Manga quotes : Photo <PERSON>, Manga Quotes, Art Images, Fan Art, Thoughts, Think, Anime, Crying, Random\n",
      "{'image_emb': [tensor([ 1.1406, -0.1748,  0.2490,  ..., -1.0547,  1.8984,  0.5781],\n",
      "       dtype=torch.bfloat16), tensor([ 0.6719, -0.1592, -1.0938,  ..., -0.9453,  1.4688,  0.8438],\n",
      "       dtype=torch.bfloat16), tensor([ 0.7617,  0.1162, -0.6523,  ..., -0.5195,  1.6172,  0.5234],\n",
      "       dtype=torch.bfloat16), tensor([ 0.2090,  0.1826, -0.2266,  ..., -0.3730,  3.7656,  0.2988],\n",
      "       dtype=torch.bfloat16), tensor([ 0.9062, -0.2090, -0.2373,  ..., -0.7148,  1.1719,  0.2949],\n",
      "       dtype=torch.bfloat16)], 'text_emb': [tensor([ 0.5195, -0.1592,  0.2441,  ..., -0.3887,  0.6914,  0.6250],\n",
      "       dtype=torch.bfloat16), tensor([ 0.6641,  0.4570,  0.7148,  ..., -0.3730,  2.1406,  0.3828],\n",
      "       dtype=torch.bfloat16), tensor([ 0.3398, -0.3750,  0.8359,  ..., -1.1016,  2.2500,  0.5547],\n",
      "       dtype=torch.bfloat16), tensor([ 0.7109,  0.0386,  0.7461,  ..., -0.4453,  2.8906,  0.4102],\n",
      "       dtype=torch.bfloat16)]}\n",
      "tensor([0.6719], dtype=torch.bfloat16)\n",
      "tensor([0.4668], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "import webdataset as wds\n",
    "import torch\n",
    "import io\n",
    "from torch.nn.functional import cosine_similarity\n",
    "dataset = wds.WebDataset(\"/home/user/data/cc12m_fixsyn_dr/00000.tar\")\n",
    "for sample in dataset:\n",
    "    print(sample['paug.json'].decode('utf-8'))\n",
    "    print(sample[\"syn_caps.json\"].decode('utf-8'))\n",
    "    print(sample[\"txt\"].decode('utf-8'))\n",
    "    buffer = io.BytesIO(sample['pth'])\n",
    "    tensor = torch.load(buffer)\n",
    "    print(tensor)\n",
    "    print(cosine_similarity(tensor['text_emb'][2].unsqueeze(0),tensor['text_emb'][3].unsqueeze(0)))\n",
    "    print(cosine_similarity(tensor['image_emb'][1].unsqueeze(0),tensor['image_emb'][3].unsqueeze(0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.load('/home/user/data/cc12m_dr/01000/010000001.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_emb': [tensor([ 0.5820,  0.8750, -0.3301,  ..., -0.0483, -2.2031,  1.0078],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([ 0.2734,  1.2500,  0.1289,  ...,  0.0327, -1.1406,  0.8008],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([-0.0864,  0.4375,  0.0598,  ..., -0.3867, -2.2344,  0.4355],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([ 0.4707,  1.0781, -0.1455,  ..., -0.0806, -2.2500,  1.0781],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([ 0.2676,  0.6055, -0.0618,  ...,  0.0354,  0.1963,  0.3945],\n",
       "         dtype=torch.bfloat16)],\n",
       " 'text_emb': [tensor([-0.5234,  0.1318, -0.6914,  ...,  0.0015, -0.3594,  0.5117],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([-0.3516,  0.4219,  0.5117,  ...,  0.3438,  0.0796,  0.9102],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([-0.7812, -0.2852,  0.0025,  ..., -0.8047,  0.6875,  0.0942],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([-0.3145,  0.0688, -0.0688,  ..., -0.5000,  1.1562, -0.0942],\n",
       "         dtype=torch.bfloat16)]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor['image_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['image_emb'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor['text_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/user/data/DataCompDR/DataCompDR-12M-bf16/00000000/5252820736bc680b29c6dca972776dc3.paug.json','r') as f:\n",
    "    dic = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(dic['param_aug']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(dic['param_aug'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33, 29, 48, 57], [[13, 0.0], [11, 178.5]]]\n"
     ]
    }
   ],
   "source": [
    "print(dic['param_aug'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import torch\n",
    "\n",
    "with gzip.open('/home/user/data/DataCompDR/DataCompDR-12M-bf16/00000000/5252820736bc680b29c6dca972776dc3.pth.gz', 'rb') as f:\n",
    "    tensor = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor['text_emb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "print(tensor['text_emb'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "print(tensor['image_emb'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0037, -0.0552, -0.0244,  ..., -0.0537, -0.0037,  0.0378],\n",
       "        [ 0.0037, -0.0552, -0.0244,  ..., -0.0537, -0.0037,  0.0378],\n",
       "        [ 0.0408, -0.0261,  0.0214,  ..., -0.0776, -0.0013, -0.0064],\n",
       "        ...,\n",
       "        [-0.0272, -0.0254,  0.0016,  ..., -0.0479,  0.0339,  0.0535],\n",
       "        [-0.0109,  0.0195,  0.0005,  ..., -0.0520, -0.0298,  0.0072],\n",
       "        [-0.0186, -0.0315, -0.0067,  ..., -0.0737,  0.0508,  0.0366]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0037, -0.0552, -0.0244,  ..., -0.0537, -0.0037,  0.0378],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0037, -0.0552, -0.0244,  ..., -0.0537, -0.0037,  0.0378],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0408, -0.0261,  0.0214,  ..., -0.0776, -0.0013, -0.0064],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][0] == tensor['text_emb'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (tensor['text_emb'][0] == tensor['text_emb'][1]):\n",
    "    if i == True:\n",
    "        continue\n",
    "    else:\n",
    "        print(False)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5859, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(tensor['text_emb'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.nn.functional.normalize(tensor['text_emb'][0],dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
