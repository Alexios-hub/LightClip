{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
       " ('ViT-B-32-256', 'datacomp_s34b_b86k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'dfn2b'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'dfn2b'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-H-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-H-14-quickgelu', 'dfn5b'),\n",
       " ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
       " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
       " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
       " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
       " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
       " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
       " ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
       " ('ViT-B-16-SigLIP', 'webli'),\n",
       " ('ViT-B-16-SigLIP-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP-384', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
       " ('ViT-L-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-L-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA-336', 'laion2b'),\n",
       " ('ViT-H-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
       " ('nllb-clip-base', 'v1'),\n",
       " ('nllb-clip-large', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'v1'),\n",
       " ('nllb-clip-large-siglip', 'v1')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import create_model_and_transforms, get_tokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer = get_tokenizer(model_name=\"ViT-L-14\")\n",
    "openai_clip, openai_preprocess_train, openai_preprocess_val = create_model_and_transforms(\n",
    "    model_name=\"ViT-L-14\",\n",
    "    pretrained='openai',\n",
    "    precision=\"bf16\"\n",
    ")\n",
    "openai_clip = openai_clip.to(device)\n",
    "    \n",
    "datacomp_clip, datacomp_preprocess_train, datacomp_preprocess_val = create_model_and_transforms(\n",
    "    model_name=\"ViT-L-14\",\n",
    "    pretrained='/home/user/data/LightClip/CLIP-KD/pretrained_models/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/open_clip_pytorch_model.bin',\n",
    "    precision=\"bf16\"\n",
    ")\n",
    "datacomp_clip = datacomp_clip.to(device)\n",
    "\n",
    "preprocess = openai_preprocess_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(4.6052, device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_clip.logit_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(4.6052, device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacomp_clip.logit_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('/home/user/data/test_cap.png')\n",
    "img1 = openai_preprocess_val(image).type(torch.bfloat16).to(device).unsqueeze(0)\n",
    "img2 = datacomp_preprocess_val(image).type(torch.bfloat16).to(device).unsqueeze(0)\n",
    "\n",
    "text = tokenizer([\"A Parent's Guide to Special Education logo.\",\"a pretty girl in black\", \"a parent 's guide to special education \", \"a parent 's guide to special education \", \"a parent 's guide to special education \"]).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 77])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49406,   320, 10183,   568,  3555,   531,  1689,  2806,  5750,   269,\n",
       "        49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = openai_clip(img1,text)\n",
    "output2 = datacomp_clip(img2,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1f,t1f,logit_exp1 = output1\n",
    "img2f,t2f,logit_exp2 = output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1f /= img1f.norm(dim=-1,keepdim=True)\n",
    "img2f /= img2f.norm(dim=-1,keepdim=True)\n",
    "t1f /= t1f.norm(dim=-1,keepdim=True)\n",
    "t2f /= t2f.norm(dim=-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1 = (100.0*img1f@t1f.T).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3097e-07, 1.0000e+00, 1.7929e-04, 1.7929e-04, 1.7929e-04]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[8, 36, 226, 207], [[3, 30.45317268371582], [8, -0.26999998092651367]]], [[152, 29, 102, 93], [[3, -30.45317268371582], [13, 0.0]]], [[81, 9, 134, 117], [[6, 0.26999998092651367], [4, -30.45317268371582]]], [[85, 18, 85, 84], [[9, 0.26999998092651367], [13, 0.0]]], [[9, 3, 236, 251], [[10, 7.0], [5, -9.0]]]]\n",
      "[\"a man is holding a green object and a quote \", \"a man holding a ball with a quote about it . \", \"a man holding a tennis ball in a field . \"]\n",
      "The source of Anime quotes & Manga quotes : Photo <PERSON>, Manga Quotes, Art Images, Fan Art, Thoughts, Think, Anime, Crying, Random\n",
      "{'image_emb': [tensor([ 1.1406, -0.1748,  0.2490,  ..., -1.0547,  1.8984,  0.5781],\n",
      "       dtype=torch.bfloat16), tensor([ 0.6719, -0.1592, -1.0938,  ..., -0.9453,  1.4688,  0.8438],\n",
      "       dtype=torch.bfloat16), tensor([ 0.7617,  0.1162, -0.6523,  ..., -0.5195,  1.6172,  0.5234],\n",
      "       dtype=torch.bfloat16), tensor([ 0.2090,  0.1826, -0.2266,  ..., -0.3730,  3.7656,  0.2988],\n",
      "       dtype=torch.bfloat16), tensor([ 0.9062, -0.2090, -0.2373,  ..., -0.7148,  1.1719,  0.2949],\n",
      "       dtype=torch.bfloat16)], 'text_emb': [tensor([ 0.5195, -0.1592,  0.2441,  ..., -0.3887,  0.6914,  0.6250],\n",
      "       dtype=torch.bfloat16), tensor([ 0.0024,  0.1523,  0.3555,  ..., -1.3750,  0.0413,  0.6641],\n",
      "       dtype=torch.bfloat16), tensor([-0.4355,  0.8281,  0.2207,  ..., -0.0952,  0.7227,  0.1719],\n",
      "       dtype=torch.bfloat16), tensor([ 0.1846, -0.0200, -0.1875,  ..., -0.7461,  0.3008,  0.1250],\n",
      "       dtype=torch.bfloat16)]}\n",
      "tensor([0.1943], dtype=torch.bfloat16)\n",
      "tensor([0.4668], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "import webdataset as wds\n",
    "import torch\n",
    "import io\n",
    "from torch.nn.functional import cosine_similarity\n",
    "dataset = wds.WebDataset(\"/home/user/data/cc12m_dr/00000.tar\")\n",
    "for sample in dataset:\n",
    "    print(sample['paug.json'].decode('utf-8'))\n",
    "    print(sample[\"syn_caps.json\"].decode('utf-8'))\n",
    "    print(sample[\"txt\"].decode('utf-8'))\n",
    "    buffer = io.BytesIO(sample['pth'])\n",
    "    tensor = torch.load(buffer)\n",
    "    print(tensor)\n",
    "    print(cosine_similarity(tensor['text_emb'][2].unsqueeze(0),tensor['text_emb'][3].unsqueeze(0)))\n",
    "    print(cosine_similarity(tensor['image_emb'][1].unsqueeze(0),tensor['image_emb'][3].unsqueeze(0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[8, 36, 226, 207], [[3, 30.45317268371582], [8, -0.26999998092651367]]], [[152, 29, 102, 93], [[3, -30.45317268371582], [13, 0.0]]], [[81, 9, 134, 117], [[6, 0.26999998092651367], [4, -30.45317268371582]]], [[85, 18, 85, 84], [[9, 0.26999998092651367], [13, 0.0]]], [[9, 3, 236, 251], [[10, 7.0], [5, -9.0]]]]\n",
      "[\"a man is holding a green object and a quote \", \"a man holding a ball with a quote about it . \", \"a man holding a tennis ball in a field . \"]\n",
      "The source of Anime quotes & Manga quotes : Photo <PERSON>, Manga Quotes, Art Images, Fan Art, Thoughts, Think, Anime, Crying, Random\n",
      "{'image_emb': [tensor([ 1.1406, -0.1748,  0.2490,  ..., -1.0547,  1.8984,  0.5781],\n",
      "       dtype=torch.bfloat16), tensor([ 0.6719, -0.1592, -1.0938,  ..., -0.9453,  1.4688,  0.8438],\n",
      "       dtype=torch.bfloat16), tensor([ 0.7617,  0.1162, -0.6523,  ..., -0.5195,  1.6172,  0.5234],\n",
      "       dtype=torch.bfloat16), tensor([ 0.2090,  0.1826, -0.2266,  ..., -0.3730,  3.7656,  0.2988],\n",
      "       dtype=torch.bfloat16), tensor([ 0.9062, -0.2090, -0.2373,  ..., -0.7148,  1.1719,  0.2949],\n",
      "       dtype=torch.bfloat16)], 'text_emb': [tensor([ 0.5195, -0.1592,  0.2441,  ..., -0.3887,  0.6914,  0.6250],\n",
      "       dtype=torch.bfloat16), tensor([ 0.6641,  0.4570,  0.7148,  ..., -0.3730,  2.1406,  0.3828],\n",
      "       dtype=torch.bfloat16), tensor([ 0.3398, -0.3750,  0.8359,  ..., -1.1016,  2.2500,  0.5547],\n",
      "       dtype=torch.bfloat16), tensor([ 0.7109,  0.0386,  0.7461,  ..., -0.4453,  2.8906,  0.4102],\n",
      "       dtype=torch.bfloat16)]}\n",
      "tensor([0.6719], dtype=torch.bfloat16)\n",
      "tensor([0.4668], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "import webdataset as wds\n",
    "import torch\n",
    "import io\n",
    "from torch.nn.functional import cosine_similarity\n",
    "dataset = wds.WebDataset(\"/home/user/data/cc12m_fixsyn_dr/00000.tar\")\n",
    "for sample in dataset:\n",
    "    print(sample['paug.json'].decode('utf-8'))\n",
    "    print(sample[\"syn_caps.json\"].decode('utf-8'))\n",
    "    print(sample[\"txt\"].decode('utf-8'))\n",
    "    buffer = io.BytesIO(sample['pth'])\n",
    "    tensor = torch.load(buffer)\n",
    "    print(tensor)\n",
    "    print(cosine_similarity(tensor['text_emb'][2].unsqueeze(0),tensor['text_emb'][3].unsqueeze(0)))\n",
    "    print(cosine_similarity(tensor['image_emb'][1].unsqueeze(0),tensor['image_emb'][3].unsqueeze(0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.load('/home/user/data/cc12m_fixsyn_dr/00000/000009999.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_emb': [tensor([-0.0859,  0.6602,  0.6094,  ..., -0.6562,  0.4609,  1.2266],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([ 0.5312,  0.3496,  0.8711,  ..., -1.0312,  2.4531,  0.1914],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([-0.0371,  0.1074,  0.6367,  ..., -0.9180,  2.2188,  0.5078],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([ 0.9062,  0.6133,  0.0977,  ..., -1.5703, -0.3867, -0.2715],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([ 0.3359,  0.1641,  0.1748,  ..., -1.3359,  2.0156, -0.3594],\n",
       "         dtype=torch.bfloat16)],\n",
       " 'text_emb': [tensor([-0.5820, -0.3320, -0.0957,  ..., -0.5234,  0.2266, -1.5859],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([ 0.2598, -0.0835,  0.4453,  ..., -0.6406,  1.8047, -0.5312],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([-0.1377,  0.0830,  0.4062,  ..., -0.5000,  1.3672, -0.4707],\n",
       "         dtype=torch.bfloat16),\n",
       "  tensor([ 0.1406, -0.3301,  0.4219,  ..., -0.6211,  2.0781, -0.4102],\n",
       "         dtype=torch.bfloat16)]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor['image_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['image_emb'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor['text_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/user/data/DataCompDR/DataCompDR-12M-bf16/00000000/5252820736bc680b29c6dca972776dc3.paug.json','r') as f:\n",
    "    dic = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(dic['param_aug']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(dic['param_aug'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33, 29, 48, 57], [[13, 0.0], [11, 178.5]]]\n"
     ]
    }
   ],
   "source": [
    "print(dic['param_aug'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import torch\n",
    "\n",
    "with gzip.open('/home/user/data/DataCompDR/DataCompDR-12M-bf16/00000000/5252820736bc680b29c6dca972776dc3.pth.gz', 'rb') as f:\n",
    "    tensor = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor['text_emb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "print(tensor['text_emb'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "print(tensor['image_emb'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0037, -0.0552, -0.0244,  ..., -0.0537, -0.0037,  0.0378],\n",
       "        [ 0.0037, -0.0552, -0.0244,  ..., -0.0537, -0.0037,  0.0378],\n",
       "        [ 0.0408, -0.0261,  0.0214,  ..., -0.0776, -0.0013, -0.0064],\n",
       "        ...,\n",
       "        [-0.0272, -0.0254,  0.0016,  ..., -0.0479,  0.0339,  0.0535],\n",
       "        [-0.0109,  0.0195,  0.0005,  ..., -0.0520, -0.0298,  0.0072],\n",
       "        [-0.0186, -0.0315, -0.0067,  ..., -0.0737,  0.0508,  0.0366]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0037, -0.0552, -0.0244,  ..., -0.0537, -0.0037,  0.0378],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0037, -0.0552, -0.0244,  ..., -0.0537, -0.0037,  0.0378],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0408, -0.0261,  0.0214,  ..., -0.0776, -0.0013, -0.0064],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor['text_emb'][0] == tensor['text_emb'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (tensor['text_emb'][0] == tensor['text_emb'][1]):\n",
    "    if i == True:\n",
    "        continue\n",
    "    else:\n",
    "        print(False)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5859, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(tensor['text_emb'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.nn.functional.normalize(tensor['text_emb'][0],dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
