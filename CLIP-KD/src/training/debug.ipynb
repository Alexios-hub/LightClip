{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:swin_tiny_patch4_window7_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.conda/envs/clipenv/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "module_path = os.path.abspath('/home/alex/data/LightClip/CLIP-KD')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.open_clip import create_model,AppleMobileCLIP\n",
    "from src.open_clip.transform import image_transform\n",
    "from src.open_clip import get_tokenizer\n",
    "\n",
    "import mobileclip\n",
    "from mobileclip.models.mci import ParallelAttentionBlock\n",
    "from PIL import  Image\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = create_model(model_name=\"timm-swin_tiny_patch4_window7_224\",device=torch.device(\"cuda\"))\n",
    "\n",
    "tokenizer_openclip = get_tokenizer(\"timm-swin_tiny_patch4_window7_224\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_model, _, preprocess = mobileclip.create_model_and_transforms('mobileclip_s0', pretrained='/home/alex/data/LightClip/ml-mobileclip/checkpoints/mobileclip_s0.pt')\n",
    "preprocess_val = image_transform(256,is_train=False, mean=[0.48145466,0.4578275,0.40821073], std=[0.26862954,0.26130258,0.27577711])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
      "    CenterCrop(size=(256, 256))\n",
      "    ToTensor()\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)\n",
      "    CenterCrop(size=(256, 256))\n",
      "    <function _convert_to_rgb at 0x75d941f6ad40>\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(preprocess)\n",
    "print(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "preprocess_val = transforms.Compose([\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(256, 256)),\n",
    "    transforms.Lambda(lambda image: image.convert('RGB')),  # Assuming _convert_to_rgb is this\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[4.3204e-06, 1.4085e-04, 4.5825e-04, 9.9940e-01]], device='cuda:0')\n",
      "4.320384960010415e-06\n",
      "0.00014085338625591248\n",
      "0.00045824635890312493\n",
      "0.999396562576294\n"
     ]
    }
   ],
   "source": [
    "mobile_model = mobile_model.to(device)\n",
    "image = preprocess(Image.open(\"/home/alex/data/LightClip/ml-mobileclip/docs/test_cap.png\").convert('RGB')).unsqueeze(0).half().to(device)\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "\n",
    "text = tokenizer([\"a photo of diagram\", \"a photo of dog\", \"a photo of cat\", \"a photo of girl\"]).to(device)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = mobile_model.encode_image(image)\n",
    "    text_features = mobile_model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)\n",
    "for prob in text_probs[0]:\n",
    "    print(prob.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1212, 0.1565, 0.1683, 0.2461]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features @ text_features.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:swin_tiny_patch4_window7_224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AppleMobileCLIP(**(model.init_params)).to(device)\n",
    "del model.visual\n",
    "model.visual = mobile_model.image_encoder.to(device)\n",
    "model.visual.model.network[7][0] = ParallelAttentionBlock(**model.visual.model.network[7][0].init_params).to(device)\n",
    "model.visual.model.network[7][1] = model.visual.model.network[7][0]\n",
    "del model.transformer\n",
    "model.transformer = mobile_model.text_encoder.to(device)\n",
    "preprocess_train = image_transform(256,is_train=True, mean=[0.48145466,0.4578275,0.40821073], std=[0.26862954,0.26130258,0.27577711])\n",
    "preprocess_val = image_transform(256,is_train=False, mean=[0.48145466,0.4578275,0.40821073], std=[0.26862954,0.26130258,0.27577711])\n",
    "del mobile_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_val = transforms.Compose([\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(256, 256)),\n",
    "    transforms.Lambda(lambda image: image.convert('RGB')),  # Assuming _convert_to_rgb is this\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[0.4369, 0.0360, 0.5271]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "image = preprocess_val(Image.open(\"/home/alex/data/LightClip/ml-mobileclip/docs/fig_accuracy_latency.png\").convert('RGB')).unsqueeze(0).half().to(device)\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/home/alex/data/LightClip/CLIP-KD/logs/2024_05_13-20_09_27-t_model_ViT-B-32-256-s_model_timm-swin_tiny_patch4_window7_224-lr_0.001-b_256-tag_distill-new/checkpoints/epoch_32.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['module.logit_scale', 'module.visual.model.patch_embed.0.reparam_conv.weight', 'module.visual.model.patch_embed.0.reparam_conv.bias', 'module.visual.model.patch_embed.1.reparam_conv.weight', 'module.visual.model.patch_embed.1.reparam_conv.bias', 'module.visual.model.patch_embed.2.reparam_conv.weight', 'module.visual.model.patch_embed.2.reparam_conv.bias', 'module.visual.model.network.0.0.layer_scale', 'module.visual.model.network.0.0.token_mixer.reparam_conv.weight', 'module.visual.model.network.0.0.token_mixer.reparam_conv.bias', 'module.visual.model.network.0.0.convffn.conv.conv.weight', 'module.visual.model.network.0.0.convffn.conv.bn.weight', 'module.visual.model.network.0.0.convffn.conv.bn.bias', 'module.visual.model.network.0.0.convffn.conv.bn.running_mean', 'module.visual.model.network.0.0.convffn.conv.bn.running_var', 'module.visual.model.network.0.0.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.0.0.convffn.fc1.weight', 'module.visual.model.network.0.0.convffn.fc1.bias', 'module.visual.model.network.0.0.convffn.fc2.weight', 'module.visual.model.network.0.0.convffn.fc2.bias', 'module.visual.model.network.0.1.layer_scale', 'module.visual.model.network.0.1.token_mixer.reparam_conv.weight', 'module.visual.model.network.0.1.token_mixer.reparam_conv.bias', 'module.visual.model.network.0.1.convffn.conv.conv.weight', 'module.visual.model.network.0.1.convffn.conv.bn.weight', 'module.visual.model.network.0.1.convffn.conv.bn.bias', 'module.visual.model.network.0.1.convffn.conv.bn.running_mean', 'module.visual.model.network.0.1.convffn.conv.bn.running_var', 'module.visual.model.network.0.1.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.0.1.convffn.fc1.weight', 'module.visual.model.network.0.1.convffn.fc1.bias', 'module.visual.model.network.0.1.convffn.fc2.weight', 'module.visual.model.network.0.1.convffn.fc2.bias', 'module.visual.model.network.1.proj.0.lkb_reparam.weight', 'module.visual.model.network.1.proj.0.lkb_reparam.bias', 'module.visual.model.network.1.proj.1.reparam_conv.weight', 'module.visual.model.network.1.proj.1.reparam_conv.bias', 'module.visual.model.network.2.0.layer_scale', 'module.visual.model.network.2.0.token_mixer.reparam_conv.weight', 'module.visual.model.network.2.0.token_mixer.reparam_conv.bias', 'module.visual.model.network.2.0.convffn.conv.conv.weight', 'module.visual.model.network.2.0.convffn.conv.bn.weight', 'module.visual.model.network.2.0.convffn.conv.bn.bias', 'module.visual.model.network.2.0.convffn.conv.bn.running_mean', 'module.visual.model.network.2.0.convffn.conv.bn.running_var', 'module.visual.model.network.2.0.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.2.0.convffn.fc1.weight', 'module.visual.model.network.2.0.convffn.fc1.bias', 'module.visual.model.network.2.0.convffn.fc2.weight', 'module.visual.model.network.2.0.convffn.fc2.bias', 'module.visual.model.network.2.1.layer_scale', 'module.visual.model.network.2.1.token_mixer.reparam_conv.weight', 'module.visual.model.network.2.1.token_mixer.reparam_conv.bias', 'module.visual.model.network.2.1.convffn.conv.conv.weight', 'module.visual.model.network.2.1.convffn.conv.bn.weight', 'module.visual.model.network.2.1.convffn.conv.bn.bias', 'module.visual.model.network.2.1.convffn.conv.bn.running_mean', 'module.visual.model.network.2.1.convffn.conv.bn.running_var', 'module.visual.model.network.2.1.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.2.1.convffn.fc1.weight', 'module.visual.model.network.2.1.convffn.fc1.bias', 'module.visual.model.network.2.1.convffn.fc2.weight', 'module.visual.model.network.2.1.convffn.fc2.bias', 'module.visual.model.network.2.2.layer_scale', 'module.visual.model.network.2.2.token_mixer.reparam_conv.weight', 'module.visual.model.network.2.2.token_mixer.reparam_conv.bias', 'module.visual.model.network.2.2.convffn.conv.conv.weight', 'module.visual.model.network.2.2.convffn.conv.bn.weight', 'module.visual.model.network.2.2.convffn.conv.bn.bias', 'module.visual.model.network.2.2.convffn.conv.bn.running_mean', 'module.visual.model.network.2.2.convffn.conv.bn.running_var', 'module.visual.model.network.2.2.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.2.2.convffn.fc1.weight', 'module.visual.model.network.2.2.convffn.fc1.bias', 'module.visual.model.network.2.2.convffn.fc2.weight', 'module.visual.model.network.2.2.convffn.fc2.bias', 'module.visual.model.network.2.3.layer_scale', 'module.visual.model.network.2.3.token_mixer.reparam_conv.weight', 'module.visual.model.network.2.3.token_mixer.reparam_conv.bias', 'module.visual.model.network.2.3.convffn.conv.conv.weight', 'module.visual.model.network.2.3.convffn.conv.bn.weight', 'module.visual.model.network.2.3.convffn.conv.bn.bias', 'module.visual.model.network.2.3.convffn.conv.bn.running_mean', 'module.visual.model.network.2.3.convffn.conv.bn.running_var', 'module.visual.model.network.2.3.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.2.3.convffn.fc1.weight', 'module.visual.model.network.2.3.convffn.fc1.bias', 'module.visual.model.network.2.3.convffn.fc2.weight', 'module.visual.model.network.2.3.convffn.fc2.bias', 'module.visual.model.network.2.4.layer_scale', 'module.visual.model.network.2.4.token_mixer.reparam_conv.weight', 'module.visual.model.network.2.4.token_mixer.reparam_conv.bias', 'module.visual.model.network.2.4.convffn.conv.conv.weight', 'module.visual.model.network.2.4.convffn.conv.bn.weight', 'module.visual.model.network.2.4.convffn.conv.bn.bias', 'module.visual.model.network.2.4.convffn.conv.bn.running_mean', 'module.visual.model.network.2.4.convffn.conv.bn.running_var', 'module.visual.model.network.2.4.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.2.4.convffn.fc1.weight', 'module.visual.model.network.2.4.convffn.fc1.bias', 'module.visual.model.network.2.4.convffn.fc2.weight', 'module.visual.model.network.2.4.convffn.fc2.bias', 'module.visual.model.network.2.5.layer_scale', 'module.visual.model.network.2.5.token_mixer.reparam_conv.weight', 'module.visual.model.network.2.5.token_mixer.reparam_conv.bias', 'module.visual.model.network.2.5.convffn.conv.conv.weight', 'module.visual.model.network.2.5.convffn.conv.bn.weight', 'module.visual.model.network.2.5.convffn.conv.bn.bias', 'module.visual.model.network.2.5.convffn.conv.bn.running_mean', 'module.visual.model.network.2.5.convffn.conv.bn.running_var', 'module.visual.model.network.2.5.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.2.5.convffn.fc1.weight', 'module.visual.model.network.2.5.convffn.fc1.bias', 'module.visual.model.network.2.5.convffn.fc2.weight', 'module.visual.model.network.2.5.convffn.fc2.bias', 'module.visual.model.network.3.proj.0.se.fc1.weight', 'module.visual.model.network.3.proj.0.se.fc1.bias', 'module.visual.model.network.3.proj.0.se.fc2.weight', 'module.visual.model.network.3.proj.0.se.fc2.bias', 'module.visual.model.network.3.proj.0.lkb_reparam.weight', 'module.visual.model.network.3.proj.0.lkb_reparam.bias', 'module.visual.model.network.3.proj.1.reparam_conv.weight', 'module.visual.model.network.3.proj.1.reparam_conv.bias', 'module.visual.model.network.4.0.layer_scale', 'module.visual.model.network.4.0.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.0.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.0.convffn.conv.conv.weight', 'module.visual.model.network.4.0.convffn.conv.bn.weight', 'module.visual.model.network.4.0.convffn.conv.bn.bias', 'module.visual.model.network.4.0.convffn.conv.bn.running_mean', 'module.visual.model.network.4.0.convffn.conv.bn.running_var', 'module.visual.model.network.4.0.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.0.convffn.fc1.weight', 'module.visual.model.network.4.0.convffn.fc1.bias', 'module.visual.model.network.4.0.convffn.fc2.weight', 'module.visual.model.network.4.0.convffn.fc2.bias', 'module.visual.model.network.4.1.layer_scale', 'module.visual.model.network.4.1.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.1.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.1.convffn.conv.conv.weight', 'module.visual.model.network.4.1.convffn.conv.bn.weight', 'module.visual.model.network.4.1.convffn.conv.bn.bias', 'module.visual.model.network.4.1.convffn.conv.bn.running_mean', 'module.visual.model.network.4.1.convffn.conv.bn.running_var', 'module.visual.model.network.4.1.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.1.convffn.fc1.weight', 'module.visual.model.network.4.1.convffn.fc1.bias', 'module.visual.model.network.4.1.convffn.fc2.weight', 'module.visual.model.network.4.1.convffn.fc2.bias', 'module.visual.model.network.4.2.layer_scale', 'module.visual.model.network.4.2.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.2.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.2.convffn.conv.conv.weight', 'module.visual.model.network.4.2.convffn.conv.bn.weight', 'module.visual.model.network.4.2.convffn.conv.bn.bias', 'module.visual.model.network.4.2.convffn.conv.bn.running_mean', 'module.visual.model.network.4.2.convffn.conv.bn.running_var', 'module.visual.model.network.4.2.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.2.convffn.fc1.weight', 'module.visual.model.network.4.2.convffn.fc1.bias', 'module.visual.model.network.4.2.convffn.fc2.weight', 'module.visual.model.network.4.2.convffn.fc2.bias', 'module.visual.model.network.4.3.layer_scale', 'module.visual.model.network.4.3.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.3.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.3.convffn.conv.conv.weight', 'module.visual.model.network.4.3.convffn.conv.bn.weight', 'module.visual.model.network.4.3.convffn.conv.bn.bias', 'module.visual.model.network.4.3.convffn.conv.bn.running_mean', 'module.visual.model.network.4.3.convffn.conv.bn.running_var', 'module.visual.model.network.4.3.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.3.convffn.fc1.weight', 'module.visual.model.network.4.3.convffn.fc1.bias', 'module.visual.model.network.4.3.convffn.fc2.weight', 'module.visual.model.network.4.3.convffn.fc2.bias', 'module.visual.model.network.4.4.layer_scale', 'module.visual.model.network.4.4.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.4.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.4.convffn.conv.conv.weight', 'module.visual.model.network.4.4.convffn.conv.bn.weight', 'module.visual.model.network.4.4.convffn.conv.bn.bias', 'module.visual.model.network.4.4.convffn.conv.bn.running_mean', 'module.visual.model.network.4.4.convffn.conv.bn.running_var', 'module.visual.model.network.4.4.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.4.convffn.fc1.weight', 'module.visual.model.network.4.4.convffn.fc1.bias', 'module.visual.model.network.4.4.convffn.fc2.weight', 'module.visual.model.network.4.4.convffn.fc2.bias', 'module.visual.model.network.4.5.layer_scale', 'module.visual.model.network.4.5.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.5.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.5.convffn.conv.conv.weight', 'module.visual.model.network.4.5.convffn.conv.bn.weight', 'module.visual.model.network.4.5.convffn.conv.bn.bias', 'module.visual.model.network.4.5.convffn.conv.bn.running_mean', 'module.visual.model.network.4.5.convffn.conv.bn.running_var', 'module.visual.model.network.4.5.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.5.convffn.fc1.weight', 'module.visual.model.network.4.5.convffn.fc1.bias', 'module.visual.model.network.4.5.convffn.fc2.weight', 'module.visual.model.network.4.5.convffn.fc2.bias', 'module.visual.model.network.4.6.layer_scale', 'module.visual.model.network.4.6.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.6.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.6.convffn.conv.conv.weight', 'module.visual.model.network.4.6.convffn.conv.bn.weight', 'module.visual.model.network.4.6.convffn.conv.bn.bias', 'module.visual.model.network.4.6.convffn.conv.bn.running_mean', 'module.visual.model.network.4.6.convffn.conv.bn.running_var', 'module.visual.model.network.4.6.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.6.convffn.fc1.weight', 'module.visual.model.network.4.6.convffn.fc1.bias', 'module.visual.model.network.4.6.convffn.fc2.weight', 'module.visual.model.network.4.6.convffn.fc2.bias', 'module.visual.model.network.4.7.layer_scale', 'module.visual.model.network.4.7.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.7.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.7.convffn.conv.conv.weight', 'module.visual.model.network.4.7.convffn.conv.bn.weight', 'module.visual.model.network.4.7.convffn.conv.bn.bias', 'module.visual.model.network.4.7.convffn.conv.bn.running_mean', 'module.visual.model.network.4.7.convffn.conv.bn.running_var', 'module.visual.model.network.4.7.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.7.convffn.fc1.weight', 'module.visual.model.network.4.7.convffn.fc1.bias', 'module.visual.model.network.4.7.convffn.fc2.weight', 'module.visual.model.network.4.7.convffn.fc2.bias', 'module.visual.model.network.4.8.layer_scale', 'module.visual.model.network.4.8.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.8.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.8.convffn.conv.conv.weight', 'module.visual.model.network.4.8.convffn.conv.bn.weight', 'module.visual.model.network.4.8.convffn.conv.bn.bias', 'module.visual.model.network.4.8.convffn.conv.bn.running_mean', 'module.visual.model.network.4.8.convffn.conv.bn.running_var', 'module.visual.model.network.4.8.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.8.convffn.fc1.weight', 'module.visual.model.network.4.8.convffn.fc1.bias', 'module.visual.model.network.4.8.convffn.fc2.weight', 'module.visual.model.network.4.8.convffn.fc2.bias', 'module.visual.model.network.4.9.layer_scale', 'module.visual.model.network.4.9.token_mixer.reparam_conv.weight', 'module.visual.model.network.4.9.token_mixer.reparam_conv.bias', 'module.visual.model.network.4.9.convffn.conv.conv.weight', 'module.visual.model.network.4.9.convffn.conv.bn.weight', 'module.visual.model.network.4.9.convffn.conv.bn.bias', 'module.visual.model.network.4.9.convffn.conv.bn.running_mean', 'module.visual.model.network.4.9.convffn.conv.bn.running_var', 'module.visual.model.network.4.9.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.4.9.convffn.fc1.weight', 'module.visual.model.network.4.9.convffn.fc1.bias', 'module.visual.model.network.4.9.convffn.fc2.weight', 'module.visual.model.network.4.9.convffn.fc2.bias', 'module.visual.model.network.5.proj.0.se.fc1.weight', 'module.visual.model.network.5.proj.0.se.fc1.bias', 'module.visual.model.network.5.proj.0.se.fc2.weight', 'module.visual.model.network.5.proj.0.se.fc2.bias', 'module.visual.model.network.5.proj.0.lkb_reparam.weight', 'module.visual.model.network.5.proj.0.lkb_reparam.bias', 'module.visual.model.network.5.proj.1.reparam_conv.weight', 'module.visual.model.network.5.proj.1.reparam_conv.bias', 'module.visual.model.network.6.reparam_conv.weight', 'module.visual.model.network.6.reparam_conv.bias', 'module.visual.model.network.7.0.layer_scale_1', 'module.visual.model.network.7.0.layer_scale_2', 'module.visual.model.network.7.0.norm.weight', 'module.visual.model.network.7.0.norm.bias', 'module.visual.model.network.7.0.norm.running_mean', 'module.visual.model.network.7.0.norm.running_var', 'module.visual.model.network.7.0.norm.num_batches_tracked', 'module.visual.model.network.7.0.token_mixer.qk.weight', 'module.visual.model.network.7.0.convffn.conv.conv.weight', 'module.visual.model.network.7.0.convffn.conv.bn.weight', 'module.visual.model.network.7.0.convffn.conv.bn.bias', 'module.visual.model.network.7.0.convffn.conv.bn.running_mean', 'module.visual.model.network.7.0.convffn.conv.bn.running_var', 'module.visual.model.network.7.0.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.7.0.convffn.fc1.weight', 'module.visual.model.network.7.0.convffn.fc1.bias', 'module.visual.model.network.7.0.convffn.fc2.weight', 'module.visual.model.network.7.0.convffn.fc2.bias', 'module.visual.model.network.7.1.layer_scale_1', 'module.visual.model.network.7.1.layer_scale_2', 'module.visual.model.network.7.1.norm.weight', 'module.visual.model.network.7.1.norm.bias', 'module.visual.model.network.7.1.norm.running_mean', 'module.visual.model.network.7.1.norm.running_var', 'module.visual.model.network.7.1.norm.num_batches_tracked', 'module.visual.model.network.7.1.token_mixer.qk.weight', 'module.visual.model.network.7.1.convffn.conv.conv.weight', 'module.visual.model.network.7.1.convffn.conv.bn.weight', 'module.visual.model.network.7.1.convffn.conv.bn.bias', 'module.visual.model.network.7.1.convffn.conv.bn.running_mean', 'module.visual.model.network.7.1.convffn.conv.bn.running_var', 'module.visual.model.network.7.1.convffn.conv.bn.num_batches_tracked', 'module.visual.model.network.7.1.convffn.fc1.weight', 'module.visual.model.network.7.1.convffn.fc1.bias', 'module.visual.model.network.7.1.convffn.fc2.weight', 'module.visual.model.network.7.1.convffn.fc2.bias', 'module.visual.model.conv_exp.se.reduce.weight', 'module.visual.model.conv_exp.se.reduce.bias', 'module.visual.model.conv_exp.se.expand.weight', 'module.visual.model.conv_exp.se.expand.bias', 'module.visual.model.conv_exp.reparam_conv.weight', 'module.visual.model.conv_exp.reparam_conv.bias', 'module.visual.model.head.proj', 'module.transformer.projection_layer', 'module.transformer.embedding_layer.weight', 'module.transformer.positional_embedding.pos_embed.pos_embed', 'module.transformer.transformer.0.layer_scale', 'module.transformer.transformer.0.token_mixer.reparam_conv.weight', 'module.transformer.transformer.0.token_mixer.reparam_conv.bias', 'module.transformer.transformer.0.convffn.conv.conv.weight', 'module.transformer.transformer.0.convffn.conv.bn.weight', 'module.transformer.transformer.0.convffn.conv.bn.bias', 'module.transformer.transformer.0.convffn.conv.bn.running_mean', 'module.transformer.transformer.0.convffn.conv.bn.running_var', 'module.transformer.transformer.0.convffn.conv.bn.num_batches_tracked', 'module.transformer.transformer.0.convffn.fc1.weight', 'module.transformer.transformer.0.convffn.fc1.bias', 'module.transformer.transformer.0.convffn.fc2.weight', 'module.transformer.transformer.0.convffn.fc2.bias', 'module.transformer.transformer.1.pre_norm_mha.0.weight', 'module.transformer.transformer.1.pre_norm_mha.0.bias', 'module.transformer.transformer.1.pre_norm_mha.1.qkv_proj.weight', 'module.transformer.transformer.1.pre_norm_mha.1.qkv_proj.bias', 'module.transformer.transformer.1.pre_norm_mha.1.out_proj.weight', 'module.transformer.transformer.1.pre_norm_mha.1.out_proj.bias', 'module.transformer.transformer.1.pre_norm_ffn.0.weight', 'module.transformer.transformer.1.pre_norm_ffn.0.bias', 'module.transformer.transformer.1.pre_norm_ffn.1.weight', 'module.transformer.transformer.1.pre_norm_ffn.1.bias', 'module.transformer.transformer.1.pre_norm_ffn.4.weight', 'module.transformer.transformer.1.pre_norm_ffn.4.bias', 'module.transformer.transformer.2.pre_norm_mha.0.weight', 'module.transformer.transformer.2.pre_norm_mha.0.bias', 'module.transformer.transformer.2.pre_norm_mha.1.qkv_proj.weight', 'module.transformer.transformer.2.pre_norm_mha.1.qkv_proj.bias', 'module.transformer.transformer.2.pre_norm_mha.1.out_proj.weight', 'module.transformer.transformer.2.pre_norm_mha.1.out_proj.bias', 'module.transformer.transformer.2.pre_norm_ffn.0.weight', 'module.transformer.transformer.2.pre_norm_ffn.0.bias', 'module.transformer.transformer.2.pre_norm_ffn.1.weight', 'module.transformer.transformer.2.pre_norm_ffn.1.bias', 'module.transformer.transformer.2.pre_norm_ffn.4.weight', 'module.transformer.transformer.2.pre_norm_ffn.4.bias', 'module.transformer.transformer.3.pre_norm_mha.0.weight', 'module.transformer.transformer.3.pre_norm_mha.0.bias', 'module.transformer.transformer.3.pre_norm_mha.1.qkv_proj.weight', 'module.transformer.transformer.3.pre_norm_mha.1.qkv_proj.bias', 'module.transformer.transformer.3.pre_norm_mha.1.out_proj.weight', 'module.transformer.transformer.3.pre_norm_mha.1.out_proj.bias', 'module.transformer.transformer.3.pre_norm_ffn.0.weight', 'module.transformer.transformer.3.pre_norm_ffn.0.bias', 'module.transformer.transformer.3.pre_norm_ffn.1.weight', 'module.transformer.transformer.3.pre_norm_ffn.1.bias', 'module.transformer.transformer.3.pre_norm_ffn.4.weight', 'module.transformer.transformer.3.pre_norm_ffn.4.bias', 'module.transformer.transformer.4.pre_norm_mha.0.weight', 'module.transformer.transformer.4.pre_norm_mha.0.bias', 'module.transformer.transformer.4.pre_norm_mha.1.qkv_proj.weight', 'module.transformer.transformer.4.pre_norm_mha.1.qkv_proj.bias', 'module.transformer.transformer.4.pre_norm_mha.1.out_proj.weight', 'module.transformer.transformer.4.pre_norm_mha.1.out_proj.bias', 'module.transformer.transformer.4.pre_norm_ffn.0.weight', 'module.transformer.transformer.4.pre_norm_ffn.0.bias', 'module.transformer.transformer.4.pre_norm_ffn.1.weight', 'module.transformer.transformer.4.pre_norm_ffn.1.bias', 'module.transformer.transformer.4.pre_norm_ffn.4.weight', 'module.transformer.transformer.4.pre_norm_ffn.4.bias', 'module.transformer.transformer.5.layer_scale', 'module.transformer.transformer.5.token_mixer.reparam_conv.weight', 'module.transformer.transformer.5.token_mixer.reparam_conv.bias', 'module.transformer.transformer.5.convffn.conv.conv.weight', 'module.transformer.transformer.5.convffn.conv.bn.weight', 'module.transformer.transformer.5.convffn.conv.bn.bias', 'module.transformer.transformer.5.convffn.conv.bn.running_mean', 'module.transformer.transformer.5.convffn.conv.bn.running_var', 'module.transformer.transformer.5.convffn.conv.bn.num_batches_tracked', 'module.transformer.transformer.5.convffn.fc1.weight', 'module.transformer.transformer.5.convffn.fc1.bias', 'module.transformer.transformer.5.convffn.fc2.weight', 'module.transformer.transformer.5.convffn.fc2.bias', 'module.transformer.final_layer_norm.weight', 'module.transformer.final_layer_norm.bias'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if k.startswith(\"module.\"):\n",
    "        new_state_dict[k[7:]] = v  # 去掉 \"module.\" 前缀\n",
    "    else:\n",
    "        new_state_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[2.0122e-05, 4.9518e-04, 6.9832e-04, 9.9879e-01]], device='cuda:0')\n",
      "2.012164804909844e-05\n",
      "0.0004951799055561423\n",
      "0.000698315619956702\n",
      "0.9987863898277283\n"
     ]
    }
   ],
   "source": [
    "image = preprocess_val(Image.open(\"/home/alex/data/LightClip/ml-mobileclip/docs/test_cap.png\").convert('RGB')).unsqueeze(0).half().to(device)\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "\n",
    "text = tokenizer([\"a photo of diagram\", \"a photo of dog\", \"a photo of cat\", \"a photo of girl\"]).to(device)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)\n",
    "for prob in text_probs[0]:\n",
    "    print(prob.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[2.3136e-05, 9.6852e-04, 1.2436e-03, 9.9776e-01]], device='cuda:0')\n",
      "2.3136184609029442e-05\n",
      "0.000968523439951241\n",
      "0.0012436088873073459\n",
      "0.9977647066116333\n"
     ]
    }
   ],
   "source": [
    "preprocess_val = image_transform(256,is_train=False, mean=[0.48145466,0.4578275,0.40821073], std=[0.26862954,0.26130258,0.27577711])\n",
    "image = preprocess_val(Image.open(\"/home/alex/data/LightClip/ml-mobileclip/docs/test_cap.png\").convert('RGB')).unsqueeze(0).half().to(device)\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "\n",
    "text = tokenizer([\"a photo of diagram\", \"a photo of dog\", \"a photo of cat\", \"a photo of girl\"]).to(device)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)\n",
    "for prob in text_probs[0]:\n",
    "    print(prob.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features, text_features, logit_scale = model(image.float(),text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2544e-05, 9.6849e-04, 1.2010e-03, 9.9781e-01]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_probs = (logit_scale.mean() * image_features @ text_features.t()).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.5194],\n",
       "        [12.3643],\n",
       "        [12.4699],\n",
       "        [15.7681]], device='cuda:0', grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49.0637, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_scale.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2144, 0.2520, 0.2542, 0.3214]], device='cuda:0',\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features @ text_features.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2144, 0.2520, 0.2542, 0.3214]], device='cuda:0',\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features @ text_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2386, 0.2477, 0.2482, 0.2655]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(image_features @ text_features.T).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2544e-05, 9.6849e-04, 1.2010e-03, 9.9781e-01]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100.0 * image_features @ text_features.T).softmax(dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
