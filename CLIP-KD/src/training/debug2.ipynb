{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:swin_tiny_patch4_window7_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.conda/envs/clipenv/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "module_path = os.path.abspath('/home/alex/data/LightClip/CLIP-KD')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.open_clip import create_model,AppleMobileCLIP\n",
    "from src.open_clip.transform import image_transform\n",
    "from src.open_clip import get_tokenizer\n",
    "\n",
    "import mobileclip\n",
    "from mobileclip.models.mci import ParallelAttentionBlock\n",
    "from PIL import  Image\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = create_model(model_name=\"timm-swin_tiny_patch4_window7_224\",device=torch.device(\"cuda\"))\n",
    "\n",
    "tokenizer_openclip = get_tokenizer(\"timm-swin_tiny_patch4_window7_224\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "  (visual): TimmModel(\n",
      "    (trunk): SwinTransformer(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (layers): Sequential(\n",
      "        (0): BasicLayer(\n",
      "          (blocks): Sequential(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.009)\n",
      "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): PatchMerging(\n",
      "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicLayer(\n",
      "          (blocks): Sequential(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.018)\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.027)\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): PatchMerging(\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BasicLayer(\n",
      "          (blocks): Sequential(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.036)\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.045)\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.055)\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.064)\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.073)\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.082)\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): PatchMerging(\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BasicLayer(\n",
      "          (blocks): Sequential(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.091)\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath(drop_prob=0.100)\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (head): Identity()\n",
      "    )\n",
      "    (head): Sequential(\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): ModuleList(\n",
      "      (0): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (1): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (2): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (3): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (4): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (5): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (6): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (7): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (8): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (9): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (10): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "      (11): ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 384)\n",
      "  (ln_final): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_model, _, preprocess = mobileclip.create_model_and_transforms('mobileclip_s0', pretrained='/home/alex/data/LightClip/ml-mobileclip/checkpoints/mobileclip_s0.pt')\n",
    "import torchvision.transforms as transforms\n",
    "preprocess_val = transforms.Compose([\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(256, 256)),\n",
    "    transforms.Lambda(lambda image: image.convert('RGB')),  # Assuming _convert_to_rgb is this\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:swin_tiny_patch4_window7_224\n"
     ]
    }
   ],
   "source": [
    "model = AppleMobileCLIP(**(model.init_params)).to(device)\n",
    "del model.visual\n",
    "model.visual = mobile_model.image_encoder.to(device)\n",
    "del model.transformer\n",
    "model.transformer = mobile_model.text_encoder.to(device)\n",
    "del mobile_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[1.0000e+00, 1.4368e-06, 5.1833e-07]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "image = preprocess_val(Image.open(\"/home/alex/data/LightClip/ml-mobileclip/docs/fig_accuracy_latency.png\").convert('RGB')).unsqueeze(0).half().to(device)\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=1536, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(model.visual.model.network[7][0].token_mixer.qkv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V component weights: tensor([[ 0.0226, -0.0120,  0.0077,  ...,  0.0017,  0.0294, -0.0240],\n",
      "        [-0.0075,  0.0069, -0.0092,  ..., -0.0013, -0.0381,  0.0115],\n",
      "        [ 0.0158,  0.0163, -0.0068,  ..., -0.0035,  0.0162, -0.0020],\n",
      "        ...,\n",
      "        [ 0.0015, -0.0030,  0.0181,  ..., -0.0335, -0.0156, -0.0099],\n",
      "        [-0.0021, -0.0004, -0.0186,  ...,  0.0188,  0.0167,  0.0060],\n",
      "        [ 0.0025,  0.0045,  0.0761,  ..., -0.0782,  0.0066, -0.0160]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Assuming the qkv weights are stored as (output_size, input_size * 3)\n",
    "# Split the weights into q, k, v components\n",
    "qkv_weight = model.visual.model.network[7][0].token_mixer.qkv.weight\n",
    "\n",
    "# Determine the size of each component\n",
    "dim_per_component = qkv_weight.shape[0] // 3\n",
    "# Extract the v component weights\n",
    "v_weight = qkv_weight[2*dim_per_component:3*dim_per_component]\n",
    "\n",
    "print(\"V component weights:\", v_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V component bias diagonal elements: tensor([ 2.2625e-02,  6.9320e-03, -6.7502e-03, -2.2370e-02,  1.0653e-02,\n",
      "        -7.8609e-03,  1.2283e-02,  2.9718e-03,  9.1769e-03,  8.0343e-04,\n",
      "        -2.2499e-04,  5.3979e-03, -1.2459e-02,  2.9858e-02,  7.4559e-03,\n",
      "        -4.7634e-04,  4.7319e-04, -1.0318e-02, -4.5386e-04,  1.7537e-02,\n",
      "         1.6603e-02,  1.0613e-02,  4.7535e-03, -1.3383e-02, -6.0620e-03,\n",
      "         3.5502e-02,  1.3125e-02,  3.3291e-02,  1.4827e-02,  5.3697e-03,\n",
      "         2.0350e-02,  1.5443e-02,  1.8033e-02,  3.3244e-02, -2.0086e-03,\n",
      "        -2.0147e-02,  1.2509e-02,  2.7069e-02,  1.5394e-02,  8.8825e-03,\n",
      "        -9.6251e-03,  5.8130e-04,  6.0060e-03,  2.9041e-03, -2.5074e-03,\n",
      "         1.4419e-02,  9.8336e-03, -1.7645e-02, -1.0508e-02,  2.8049e-03,\n",
      "         8.0413e-03,  4.3219e-02, -1.3540e-02,  2.6520e-04, -2.6946e-03,\n",
      "        -1.2744e-02, -6.8052e-03, -1.1126e-02, -1.1560e-03,  1.8180e-02,\n",
      "        -2.2324e-03, -8.3112e-03,  3.2682e-02,  9.3908e-03, -3.6162e-02,\n",
      "        -4.8448e-03, -8.5344e-03, -1.8756e-02,  4.6130e-02,  2.3434e-03,\n",
      "         5.9050e-03,  1.2778e-02,  1.9668e-02, -9.2464e-03,  1.1076e-02,\n",
      "        -1.3952e-02,  4.4598e-03,  1.1841e-02, -2.2466e-02, -5.4944e-04,\n",
      "        -1.2896e-02,  1.0513e-02, -2.0483e-03, -3.7526e-03,  1.1965e-03,\n",
      "        -5.1354e-03, -8.3019e-03, -1.9185e-02,  9.5033e-03, -1.4517e-03,\n",
      "        -4.3773e-03,  1.5741e-03, -8.6624e-03, -1.7104e-02, -1.5644e-02,\n",
      "         1.3056e-02, -1.5043e-03,  1.3742e-02, -1.4927e-02,  1.4896e-02,\n",
      "         1.1053e-02, -1.7504e-02, -3.4624e-02,  1.5678e-02,  1.1200e-02,\n",
      "        -3.7820e-03,  6.2586e-03, -8.2516e-03,  5.9454e-03, -2.7351e-02,\n",
      "        -2.6017e-03,  7.2353e-03,  9.5795e-03,  2.8889e-03, -1.1807e-02,\n",
      "         1.7833e-02,  3.1266e-04,  9.9037e-03,  2.4670e-02,  1.2465e-02,\n",
      "         9.5630e-03,  1.0554e-02,  2.3146e-02,  1.4111e-02,  1.8651e-02,\n",
      "        -1.0717e-02,  2.0197e-02,  2.9956e-03,  1.7280e-02, -1.3455e-02,\n",
      "        -2.1176e-02, -1.5990e-02, -1.4744e-02, -4.7794e-03,  1.1202e-02,\n",
      "         1.6898e-02,  8.8938e-03,  9.2018e-03,  1.9701e-02,  3.0529e-02,\n",
      "         1.9490e-02,  1.1663e-02,  2.4449e-02,  3.5858e-02,  2.9263e-03,\n",
      "         5.5112e-03, -2.1964e-02, -2.1386e-03, -1.1912e-03,  1.8200e-02,\n",
      "        -8.5898e-03,  4.9796e-02,  1.5239e-02,  1.0309e-02,  3.1093e-03,\n",
      "         3.5005e-02,  5.6461e-03, -2.0163e-02,  1.0230e-03,  2.4730e-02,\n",
      "        -1.6365e-03, -3.2151e-02, -4.3693e-03,  1.3144e-02, -2.6595e-03,\n",
      "        -1.0546e-02,  5.9680e-03,  1.4630e-02,  5.1718e-03,  1.8559e-03,\n",
      "         9.2124e-03,  1.5513e-02, -1.6735e-03, -1.5881e-02, -1.0038e-02,\n",
      "         1.4857e-02, -3.8980e-04,  4.3738e-02,  2.5115e-04, -1.5711e-02,\n",
      "         3.0348e-02,  7.0101e-03, -2.0558e-02,  9.5307e-03, -6.0841e-03,\n",
      "         1.3577e-03, -7.2182e-03, -1.4334e-02,  2.2325e-02,  5.0723e-03,\n",
      "        -1.9039e-02, -2.1781e-02,  3.5334e-02,  6.2578e-03, -9.1691e-03,\n",
      "         1.6192e-02, -2.3897e-02, -5.1832e-03, -1.9306e-02,  3.4991e-02,\n",
      "         1.0127e-02,  1.4602e-02, -2.8219e-03, -1.3531e-02, -1.0798e-02,\n",
      "         3.3398e-03,  2.6740e-02,  1.3323e-02, -1.9899e-02, -3.1401e-03,\n",
      "         2.5355e-03,  7.2680e-03,  2.9558e-02,  1.8067e-02,  3.3376e-03,\n",
      "         2.7826e-02,  5.2090e-03, -1.4527e-03,  3.1731e-03, -1.3091e-02,\n",
      "         2.2389e-02, -3.8932e-03,  8.9688e-03, -2.2625e-03, -2.3031e-03,\n",
      "        -7.8706e-03,  9.5621e-03,  9.5560e-03, -1.9107e-02,  1.0479e-02,\n",
      "        -4.3365e-02, -5.5929e-03, -1.5151e-02, -1.9780e-02, -3.9253e-02,\n",
      "         3.8607e-02, -1.9245e-02,  1.5312e-02, -7.8650e-03, -1.4147e-03,\n",
      "        -9.1804e-05, -4.0622e-03, -1.9901e-02,  1.6055e-03,  9.7172e-03,\n",
      "        -1.7081e-02,  1.0415e-02, -1.7901e-03, -3.6856e-03, -2.8677e-02,\n",
      "         8.1194e-03, -1.7196e-02, -3.7828e-02, -3.8824e-03, -9.1045e-03,\n",
      "         2.7214e-02, -4.3861e-03,  3.3569e-02,  2.6311e-03, -3.0643e-03,\n",
      "         2.1266e-03,  1.7502e-02,  1.1780e-02, -1.6570e-02, -2.0551e-02,\n",
      "        -3.2330e-03, -2.2635e-02,  2.0960e-02, -1.4587e-02,  2.2694e-02,\n",
      "        -6.5928e-03,  4.0895e-02, -2.0368e-04,  3.3953e-03, -4.5041e-02,\n",
      "         1.8203e-02, -2.3220e-02, -4.5598e-03, -1.2337e-02, -3.1624e-02,\n",
      "         3.7641e-03, -3.1837e-03,  1.4726e-03,  1.6347e-02, -2.1835e-02,\n",
      "        -1.0722e-02,  1.1270e-02, -1.1644e-02,  1.9116e-02,  1.0979e-02,\n",
      "        -3.0803e-02, -7.2179e-04, -2.9756e-02,  3.6158e-03, -2.2503e-02,\n",
      "         2.7997e-02, -3.4493e-02,  1.8062e-02,  1.8907e-02, -4.5282e-03,\n",
      "        -1.2019e-02,  2.7560e-03,  1.6373e-02, -2.2094e-02, -1.1049e-02,\n",
      "         6.2925e-03, -8.0054e-03, -2.0735e-02, -2.2020e-02,  1.4167e-02,\n",
      "        -9.3285e-03, -2.0978e-02,  1.7615e-02,  1.4732e-02,  4.8535e-03,\n",
      "         3.2511e-03, -5.9279e-03, -4.4946e-03,  3.1657e-03,  1.1020e-02,\n",
      "         1.6015e-02,  1.5707e-02, -6.8631e-03, -1.6730e-02, -1.4582e-02,\n",
      "        -1.6761e-02, -8.6875e-03, -1.0524e-02,  3.5103e-02,  4.0899e-03,\n",
      "         2.2114e-03,  1.6522e-02,  1.1379e-02, -5.7736e-03, -3.6056e-02,\n",
      "        -1.6314e-02,  1.3735e-02, -2.6465e-02, -2.3498e-04,  1.2648e-02,\n",
      "        -1.7901e-03, -1.6158e-02, -3.1461e-03,  1.4798e-02, -3.9782e-03,\n",
      "         6.2294e-03,  2.9620e-04,  1.6301e-02, -1.3816e-02,  1.2837e-02,\n",
      "        -4.9041e-05,  1.9497e-02,  7.0745e-03,  2.5521e-02, -2.4007e-02,\n",
      "        -2.7295e-02,  5.8669e-03, -2.0046e-02,  6.3706e-03,  2.9715e-04,\n",
      "        -5.5090e-03,  2.2859e-02, -3.3998e-02,  7.3275e-03, -1.6623e-02,\n",
      "        -6.6649e-03, -2.9313e-02,  2.7340e-04,  9.7685e-03, -2.4973e-02,\n",
      "         8.4681e-03,  1.4443e-02,  2.3222e-02,  1.4719e-02, -1.7641e-03,\n",
      "         1.1718e-02,  5.9040e-03,  2.9992e-02, -9.7441e-03, -8.6409e-04,\n",
      "         5.4915e-03, -3.8168e-03,  4.3214e-02,  4.1721e-03,  1.0722e-02,\n",
      "        -1.1506e-02,  3.8953e-03, -1.9278e-02,  7.0133e-03,  2.4424e-02,\n",
      "         5.1398e-03,  2.2165e-02, -1.3048e-02,  1.4469e-02, -1.3988e-02,\n",
      "         1.3881e-02,  3.5213e-02,  1.3598e-03, -1.3519e-02,  1.7575e-02,\n",
      "        -1.0693e-02, -5.6051e-03, -7.6337e-03,  5.3753e-03,  8.6582e-03,\n",
      "        -1.9567e-03,  5.1515e-03, -1.8835e-02,  1.4782e-02,  2.7720e-02,\n",
      "        -1.8494e-02, -7.2869e-03,  3.7484e-03, -2.5793e-02,  2.2435e-02,\n",
      "        -2.1728e-02, -4.4819e-02, -1.8011e-02,  1.2716e-02,  2.2541e-02,\n",
      "         1.5756e-02, -2.4040e-02, -1.2501e-03,  1.2409e-02, -5.9216e-03,\n",
      "         9.5155e-03,  1.1799e-02,  1.6948e-03,  1.7812e-02,  9.0287e-03,\n",
      "         1.2883e-02,  1.5806e-02,  9.7442e-03, -3.3541e-02,  1.9045e-02,\n",
      "         5.4714e-03, -1.1221e-02, -9.9363e-03,  8.4703e-03,  2.1190e-04,\n",
      "         1.0106e-03,  7.7679e-04, -1.8900e-02,  1.1632e-02, -1.0621e-02,\n",
      "         3.6633e-02,  2.7064e-02, -7.6746e-03, -1.0493e-02,  1.3569e-03,\n",
      "        -3.0360e-03,  2.3322e-03,  1.3096e-02, -6.4297e-03,  8.5610e-03,\n",
      "         7.8027e-03,  8.2427e-03,  1.9440e-04,  1.7383e-04, -8.7995e-03,\n",
      "         4.6594e-03, -9.1152e-03,  1.6544e-02,  4.0505e-02,  1.3256e-03,\n",
      "        -3.1726e-03,  1.1192e-02,  6.8741e-03, -8.4124e-03, -1.1000e-02,\n",
      "         6.3888e-03, -6.9985e-04,  3.3072e-03,  3.1615e-02,  1.4861e-03,\n",
      "         4.4324e-03,  1.3088e-02, -2.3103e-03,  3.2247e-02, -3.5333e-02,\n",
      "         7.3732e-04,  7.0102e-03, -4.1159e-03,  7.8973e-03,  4.3839e-02,\n",
      "         8.7882e-03, -1.9833e-02, -1.0921e-04, -2.9290e-02, -6.2939e-03,\n",
      "        -1.0899e-02,  1.0159e-02, -1.9439e-03, -7.4440e-03,  1.9360e-02,\n",
      "        -3.7204e-03,  6.2360e-03, -5.3877e-03, -6.4550e-03,  1.1937e-02,\n",
      "         4.3926e-03, -1.0535e-02, -1.7649e-03,  6.2192e-02,  7.7337e-03,\n",
      "        -1.1210e-02, -1.8582e-03, -9.4122e-03,  1.6549e-02, -3.3502e-02,\n",
      "         1.6664e-02, -1.6009e-02], device='cuda:0',\n",
      "       grad_fn=<DiagonalBackward0>)\n"
     ]
    }
   ],
   "source": [
    "    # Print the main diagonal elements of the v component bias\n",
    "    v_weight_diagonal = torch.diagonal(v_weight)\n",
    "    print(\"V component bias diagonal elements:\", v_weight_diagonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V component weights: tensor([[-0.0282, -0.0370, -0.0082,  ..., -0.0014,  0.0125,  0.0163],\n",
      "        [ 0.0282,  0.0197,  0.0260,  ...,  0.0127,  0.0022,  0.0008],\n",
      "        [-0.0160, -0.0287,  0.0116,  ..., -0.0184,  0.0164, -0.0282],\n",
      "        ...,\n",
      "        [-0.0005,  0.0175, -0.0311,  ..., -0.0049,  0.0168,  0.0171],\n",
      "        [ 0.0169, -0.0182,  0.0199,  ..., -0.0044, -0.0176, -0.0370],\n",
      "        [ 0.0201,  0.0190, -0.0066,  ..., -0.0210, -0.0048, -0.0021]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Assuming the qkv weights are stored as (output_size, input_size * 3)\n",
    "# Split the weights into q, k, v components\n",
    "qkv_weight = model.visual.model.network[7][1].token_mixer.qkv.weight\n",
    "\n",
    "# Determine the size of each component\n",
    "dim_per_component = qkv_weight.shape[0] // 3\n",
    "# Extract the v component weights\n",
    "v_weight = qkv_weight[2*dim_per_component:3*dim_per_component]\n",
    "\n",
    "print(\"V component weights:\", v_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V component bias diagonal elements: tensor([-2.8248e-02,  1.9723e-02,  1.1576e-02, -9.1078e-03, -5.5456e-03,\n",
      "         3.4152e-02,  3.2216e-02,  5.8315e-03, -1.7488e-02, -3.5332e-02,\n",
      "         2.0467e-02,  5.5233e-03,  2.5466e-03, -3.4352e-03, -4.7485e-03,\n",
      "        -3.0537e-02,  8.7238e-03,  3.1450e-02, -1.5156e-03, -2.9466e-04,\n",
      "         2.3092e-02, -5.2767e-04,  4.7456e-02, -8.5221e-03, -2.2598e-02,\n",
      "         2.0468e-02,  2.5689e-02, -2.5424e-02,  4.8736e-03,  1.8172e-02,\n",
      "        -4.0648e-02, -2.3025e-02,  1.9518e-02, -6.0889e-03,  4.1003e-03,\n",
      "        -9.9564e-03, -1.2454e-02, -2.2296e-02, -1.3933e-02,  1.2487e-03,\n",
      "        -1.0459e-02,  8.5141e-03,  2.3299e-02,  3.9668e-03, -9.4134e-03,\n",
      "         1.9925e-02, -1.1799e-02,  9.4941e-03, -1.0267e-03, -7.1985e-03,\n",
      "         2.9697e-02,  1.2786e-02,  4.1704e-03,  1.6534e-02, -2.3965e-02,\n",
      "        -1.4500e-03,  1.4656e-02, -9.1257e-03, -1.0409e-02, -4.2568e-04,\n",
      "         1.3144e-02,  4.4290e-03,  3.8770e-03, -2.2936e-02,  1.3292e-03,\n",
      "        -1.8537e-02,  8.9939e-03, -5.3637e-03,  4.5915e-03,  2.6696e-02,\n",
      "        -1.0542e-02, -1.1319e-02, -7.7577e-03,  2.3326e-02, -1.6312e-02,\n",
      "         2.6895e-04, -3.4182e-02, -4.5585e-02,  2.4164e-03, -3.1483e-02,\n",
      "         6.0119e-04, -2.3689e-02,  1.1510e-02, -2.4556e-02, -2.3280e-02,\n",
      "        -5.8220e-03,  3.6336e-03,  1.9028e-02,  1.0421e-02,  1.6873e-02,\n",
      "        -1.0017e-02,  1.8469e-02, -1.3442e-02, -1.0332e-02, -1.7463e-02,\n",
      "        -1.4270e-02,  1.1499e-02,  7.4700e-03, -4.0442e-03,  1.3312e-02,\n",
      "         2.0126e-02,  2.9612e-02, -7.7350e-03,  2.0236e-02,  2.4274e-03,\n",
      "         4.5099e-02,  1.5562e-02,  1.0462e-02, -2.5605e-02,  1.8310e-03,\n",
      "        -2.4500e-02,  1.7495e-02,  5.4605e-03,  2.2409e-02,  1.9964e-02,\n",
      "         9.3283e-03, -2.3204e-02, -3.3221e-02,  8.4846e-03,  1.7836e-02,\n",
      "        -7.7759e-03,  1.4928e-02, -3.5901e-03,  3.7210e-02, -9.3967e-03,\n",
      "         2.2490e-03,  2.2393e-02, -2.6003e-02,  9.4240e-03,  5.6132e-03,\n",
      "        -1.4567e-02,  1.8420e-02,  1.4634e-02, -1.1787e-02, -5.3387e-04,\n",
      "         2.5881e-02, -4.7085e-03, -1.5677e-02, -7.5133e-03,  1.4056e-02,\n",
      "         1.2945e-05,  2.5375e-02, -8.8071e-03,  6.3622e-03,  2.1602e-02,\n",
      "         1.5768e-02,  8.9549e-03,  2.7165e-02,  1.8569e-04,  9.7011e-03,\n",
      "        -3.1749e-02, -2.4557e-02,  2.3763e-02, -1.7714e-03, -3.2452e-03,\n",
      "        -1.6846e-02, -3.3350e-03,  1.9749e-02, -1.1477e-02, -6.5991e-03,\n",
      "         2.7972e-03,  6.6005e-03, -2.5439e-02,  2.0501e-02, -3.4614e-02,\n",
      "        -7.8386e-03, -4.8533e-03, -7.5716e-03,  3.9844e-03,  1.0872e-02,\n",
      "        -2.6466e-03, -2.7482e-03,  3.7063e-02,  3.1500e-02,  4.2446e-03,\n",
      "         2.8832e-02, -7.1820e-03, -2.4605e-02,  9.3494e-06, -5.4179e-03,\n",
      "         8.2884e-03,  5.4083e-03, -2.5994e-02, -1.5505e-02, -1.0528e-02,\n",
      "        -9.0037e-03, -6.0660e-03, -1.4149e-02,  9.4549e-03,  7.8544e-03,\n",
      "        -1.6370e-02, -1.4229e-02,  2.7290e-03, -3.0174e-02, -7.1805e-03,\n",
      "         3.4809e-03,  1.7362e-02, -5.0934e-03, -1.4183e-02,  9.7876e-03,\n",
      "         2.0641e-03,  1.8479e-02,  2.8474e-02,  2.3723e-02, -8.8646e-03,\n",
      "        -3.1593e-02, -1.4105e-02,  2.0190e-02, -1.4671e-02,  9.4482e-03,\n",
      "        -1.8324e-02,  7.1407e-03, -2.2115e-02, -1.1944e-02,  5.1074e-02,\n",
      "        -5.7021e-03, -1.1044e-02, -2.0497e-02, -2.8143e-03,  4.5166e-03,\n",
      "         2.2512e-02,  8.0294e-03, -3.2380e-03,  1.2981e-02,  1.9152e-02,\n",
      "         7.2055e-03,  6.7696e-03,  4.0360e-03, -1.2922e-02,  1.7062e-02,\n",
      "         2.2896e-03, -2.2621e-02,  2.6829e-02, -5.8754e-03,  1.8211e-02,\n",
      "        -1.5055e-04,  3.8910e-02, -4.1361e-02,  4.0449e-03,  1.0580e-02,\n",
      "         9.5979e-03,  5.9818e-03,  1.3416e-03, -1.1354e-02, -6.8512e-03,\n",
      "         2.3930e-02,  1.1908e-02, -2.0509e-02, -1.9579e-03, -1.7837e-02,\n",
      "        -7.5792e-03,  8.0031e-03,  5.4185e-03, -6.4428e-03, -1.0090e-02,\n",
      "        -9.1563e-03, -2.4982e-03,  1.2892e-02,  2.5077e-02, -1.2851e-02,\n",
      "        -9.6797e-03, -1.7153e-02,  2.4606e-02, -7.1708e-03,  6.0054e-03,\n",
      "        -6.3603e-03,  1.0288e-03,  2.4714e-03, -2.7222e-04,  1.4012e-02,\n",
      "         4.0099e-03, -1.0990e-03,  6.8138e-06, -8.8273e-03,  1.3425e-02,\n",
      "        -3.1134e-02,  9.1844e-03,  4.9766e-03, -3.6936e-03,  9.4761e-03,\n",
      "        -1.7674e-02, -1.1050e-02, -1.0919e-02, -1.8321e-02, -2.1381e-02,\n",
      "         2.0071e-02,  1.1932e-02, -1.6005e-02, -8.0452e-03,  1.3422e-02,\n",
      "         1.0574e-02, -2.4851e-03, -2.1288e-02,  1.4632e-03,  8.5445e-03,\n",
      "        -1.0113e-02, -4.9651e-03,  5.2021e-02, -2.5074e-02,  2.7182e-03,\n",
      "        -1.5375e-02, -3.1717e-02, -3.5376e-02,  2.0248e-02, -1.9399e-02,\n",
      "        -1.5297e-02, -4.4816e-03, -5.3505e-04,  6.1200e-03,  2.2358e-02,\n",
      "        -1.9723e-02, -2.9258e-03,  1.9037e-02,  1.2561e-02,  3.8261e-03,\n",
      "        -4.6905e-03, -1.3992e-02,  9.5916e-03,  6.3072e-03,  2.2449e-02,\n",
      "        -1.7227e-02, -2.3549e-02,  1.7888e-02,  1.3826e-03, -1.4434e-03,\n",
      "         1.7220e-02, -2.7267e-02, -1.7657e-03,  8.6792e-03, -3.3480e-03,\n",
      "        -9.5562e-03, -1.3103e-02, -1.6890e-02,  1.7596e-02,  1.0668e-02,\n",
      "         3.3457e-02, -9.5539e-03,  2.5576e-03, -3.8782e-03, -3.8863e-03,\n",
      "         2.4571e-02, -6.1386e-03, -7.3962e-04, -1.0160e-02, -4.2902e-02,\n",
      "        -1.7423e-03, -3.1741e-02,  8.2078e-03,  1.1608e-02, -7.6937e-03,\n",
      "        -8.7936e-03, -8.5923e-03, -5.2433e-03,  2.6789e-02,  9.6606e-03,\n",
      "        -2.4895e-03,  2.3962e-02, -2.1473e-02, -1.5256e-03, -1.3571e-02,\n",
      "        -8.0286e-03,  1.7580e-02, -8.1045e-03, -8.4852e-03, -1.9075e-02,\n",
      "         2.3565e-02, -1.5749e-03, -4.8661e-03,  1.1903e-02, -4.9310e-03,\n",
      "         3.1477e-02,  3.8527e-02,  7.0375e-05,  1.1417e-03,  1.7342e-02,\n",
      "        -1.1518e-02,  2.7435e-02, -5.1391e-02,  1.0395e-02,  4.1510e-03,\n",
      "        -1.5911e-03, -7.7182e-03,  1.9085e-02, -2.1082e-03, -4.1646e-02,\n",
      "         5.9964e-03,  8.4236e-03, -3.1115e-03, -4.0489e-03, -1.2721e-02,\n",
      "         3.2758e-02,  3.5432e-02, -1.9426e-02, -3.5632e-03, -2.3978e-02,\n",
      "         4.3631e-02, -1.4901e-03, -4.3400e-03,  5.8020e-03,  1.0081e-02,\n",
      "        -1.2138e-02, -2.8973e-02, -9.0802e-03, -4.2132e-02, -3.7976e-03,\n",
      "         2.3388e-03,  2.5793e-02,  1.7476e-02, -2.0888e-02,  5.6374e-04,\n",
      "         7.9799e-03, -1.1897e-02, -1.4554e-02,  2.9769e-02, -3.0019e-02,\n",
      "        -5.8496e-03, -1.0627e-03,  1.3369e-03, -9.0521e-03,  2.2411e-02,\n",
      "        -2.6234e-02,  5.9105e-04, -3.3667e-03,  5.3888e-03, -7.7849e-03,\n",
      "         1.5079e-02, -2.5071e-02, -1.8084e-02, -3.8591e-02,  3.5380e-03,\n",
      "         2.1086e-02, -2.4810e-03, -1.3887e-03,  1.1833e-02,  1.2648e-02,\n",
      "         1.9786e-02, -2.0717e-03,  1.3194e-02, -1.0530e-02,  3.2951e-03,\n",
      "         2.6196e-02,  1.6529e-03,  8.5916e-03,  1.0035e-03, -1.9118e-02,\n",
      "        -3.5844e-02,  2.1236e-02, -1.3727e-02, -1.2957e-03, -1.9006e-02,\n",
      "         9.3601e-04,  1.5054e-02,  2.1982e-02, -2.5661e-03, -3.2360e-02,\n",
      "         6.9534e-04,  1.1235e-02, -2.5785e-03, -9.6033e-03, -5.8493e-03,\n",
      "         8.1489e-03, -1.5365e-02,  1.0722e-02, -1.0781e-02, -6.5736e-03,\n",
      "        -9.3285e-04, -1.1380e-02, -4.8062e-03,  2.5350e-02, -7.0376e-03,\n",
      "        -8.5379e-03,  2.8459e-02, -2.2840e-02,  5.2324e-03,  3.6259e-02,\n",
      "         1.4986e-02, -1.0274e-02,  3.4934e-02, -3.0838e-02, -1.9468e-02,\n",
      "         1.1006e-02,  2.2849e-02,  7.3810e-03,  4.7214e-02,  2.5412e-03,\n",
      "        -7.0093e-03,  2.3099e-02, -4.4450e-03,  8.0207e-03, -1.2926e-02,\n",
      "        -5.1333e-03,  2.7058e-02, -1.0111e-02,  1.7077e-02,  3.3069e-03,\n",
      "         2.5602e-02, -3.1083e-02,  5.2249e-03,  9.3876e-03,  3.2119e-02,\n",
      "         3.3154e-03, -3.8772e-02,  6.1712e-03,  3.0488e-03,  1.3671e-02,\n",
      "         5.5561e-03,  7.5937e-03, -2.0461e-02, -5.6822e-03, -4.8784e-03,\n",
      "        -1.7622e-02, -2.0551e-03], device='cuda:0',\n",
      "       grad_fn=<DiagonalBackward0>)\n"
     ]
    }
   ],
   "source": [
    "    # Print the main diagonal elements of the v component bias\n",
    "    v_weight_diagonal = torch.diagonal(v_weight)\n",
    "    print(\"V component bias diagonal elements:\", v_weight_diagonal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
