alpha_afd_loss: 0.0
alpha_ckd_loss: 0.0
alpha_cross_kd_loss: 0.0
alpha_fd_loss: 0.0
alpha_gd_loss: 0.0
alpha_icl_loss: 0.0
batch_size: 512
beta1: 0.9
beta2: 0.999
checkpoint_path: ./logs/2024_05_31-17_21_19-t_model_['mobileclip_s0', 'ViT-L-14', 'ViT-L-14-336']-s_model_mobileclip_s0-lr_0.001-b_512-tag_grad/checkpoints
copy_codebase: False
csv_caption_key: title
csv_img_key: filepath
csv_separator: 	
data_root: /home/user/data/LightClip/datasets/cc3m/images/
dataset_resampled: False
dataset_type: csv
ddp_static_graph: False
debug: False
device: cuda:0
dist_backend: nccl
dist_url: env://
distributed: True
epochs: 32
eps: 1e-08
eval: False
force_custom_text: False
force_quick_gelu: False
gather_with_grad: False
grad_checkpointing: False
grad_clip_norm: None
horovod: False
image_mean: None
image_std: None
imagenet_a: None
imagenet_r: None
imagenet_sketch: None
imagenet_v2: None
imagenet_val: /home/user/data/LightClip/datasets/val_images/
light: False
light_version: light_mobileclip_s0
local_loss: False
local_rank: 0
lock_image: False
lock_image_freeze_bn_stats: False
lock_image_unlocked_groups: 0
lock_text: False
lock_text_freeze_layer_norm: False
lock_text_unlocked_layers: 0
log_level: 20
log_local: False
log_path: ./logs/2024_05_31-17_21_19-t_model_['mobileclip_s0', 'ViT-L-14', 'ViT-L-14-336']-s_model_mobileclip_s0-lr_0.001-b_512-tag_grad/out.log
logs: ./logs/
lr: 0.001
mask_ratio: 0.0
model: mobileclip_s0
model_checkpoint: 
name: 2024_05_31-17_21_19-t_model_['mobileclip_s0', 'ViT-L-14', 'ViT-L-14-336']-s_model_mobileclip_s0-lr_0.001-b_512-tag_grad
no_set_device_rank: False
num_per_class: 1
precision: amp
pretrained: 
pretrained_image: False
rank: 0
report_to: tensorboard
resume: None
s_embed_dim: 512
save_frequency: 1
save_most_recent: False
seed: 0
skip_scheduler: False
sup_batch_size: 64
t_embed_dim: 2048
t_eval: False
t_model_checkpoint: ['/home/user/data/LightClip/ml-mobileclip/checkpoints/mobileclip_s0.pt', '/home/user/data/LightClip/CLIP-KD/pretrained_models/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/open_clip_pytorch_model.bin', '/home/user/data/LightClip/CLIP-KD/pretrained_models/vit_large_patch14_clip_336.openai/open_clip_pytorch_model.bin']
tag: grad
teachers: ['mobileclip_s0', 'ViT-L-14', 'ViT-L-14-336']
tensorboard: True
tensorboard_path: ./logs/2024_05_31-17_21_19-t_model_['mobileclip_s0', 'ViT-L-14', 'ViT-L-14-336']-s_model_mobileclip_s0-lr_0.001-b_512-tag_grad/tensorboard
torchscript: False
trace: False
train_data: /home/user/data/LightClip/datasets/cc3m/cc3m/Train_GCC-training.csv
train_num_samples: None
use_bn_sync: False
val_data: /home/user/data/LightClip/datasets/cc3m/cc3m/Validation_GCC-1.1.0-Validation.csv
val_data_root: /home/user/data/LightClip/datasets/cc3m/images/
val_dataset_type: csv
val_frequency: 1
val_num_samples: None
vl_batch_size: 64
wandb: False
wandb_notes: 
warmup: 10000
wd: 0.1
workers: 8
world_size: 2
zeroshot_frequency: 1
